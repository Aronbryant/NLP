{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import transformers as tfs\n",
    "import torch\n",
    "from torch import nn\n",
    "from logger import Progbar\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "BASE_DATASET_PATH = 'dataset'\n",
    "dataset_path = BASE_DATASET_PATH\n",
    "POSITIVE_TRAIN_FILE_PATH = os.path.join(dataset_path, \"postive_train.json\")\n",
    "POSITIVE_TRAIN_INFO_PATH = os.path.join(dataset_path, \"positive_info.json\")\n",
    "UNLABELED_TRAIN_FILE_PATH = os.path.join(dataset_path,\"unlabeled_train.json\")\n",
    "PRETRAINED_BERT_ENCODER_PATH = 'pretrain'\n",
    "BERT_TOKENZIER_PATH = PRETRAINED_BERT_ENCODER_PATH\n",
    "BASE_MODEL_PATH = 'model'\n",
    "mdoel_path = BASE_MODEL_PATH\n",
    "FINETUNED_BERT_ENCODER_PATH = os.path.join(model_path,\"finetuned_bert.bin\")\n",
    "PU_DATA_TEXT_SAVE_PATH = os.path.join(dataset_path, \"PU_text.npy\")\n",
    "PU_DATA_LABEL_SAVE_PATH = os.path.join(dataset_path, \"PU_label.npy\")\n",
    "\n",
    "#获取一个epoch所需要的batch数\n",
    "def get_steps_per_epoch(line_count,batch_size):\n",
    "    return line_count // batch_size if line_count % batch_size == 0 else line_count +1\n",
    "# 获取数据集的标签集及其大小\n",
    "def get_label_set_and_sample_num(config_path, sample_num=False):\n",
    "    with open(config_path, \"r\", encoding=\"UTF-8\") as input_file:\n",
    "        json_data = json.loads(input_file.readline())\n",
    "        if sample_num:\n",
    "            return json_data[\"label_list\"], json_data[\"total_num\"]\n",
    "        else:\n",
    "            return json_data[\"label_list\"]\n",
    "# 定义输入到Bert中的文本的格式,即标题,正文的组织形式\n",
    "def prepare_sequence(title: str, body: str):\n",
    "    return (title, body[:256] + \"|\" + body[-256:])\n",
    "# 迭代器: 逐条读取数据并输出文本和标签\n",
    "def get_text_and_label_index_iterator(input_path):\n",
    "    with open(input_path, 'r', encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            json_data = json.loads(line)\n",
    "            text = prepare_sequence(json_data[\"title\"], json_data[\"body\"])\n",
    "            yield text\n",
    "# 迭代器: 生成一个batch的数据\n",
    "def get_bert_iterator_batch(data_path, batch_size=32):\n",
    "    keras_bert_iter = get_text_and_label_index_iterator(data_path)\n",
    "    continue_iterator = True\n",
    "    while True:\n",
    "        data_list = []\n",
    "        for _ in range(batch_size):\n",
    "            try:\n",
    "                data = next(keras_bert_iter)\n",
    "                data_list.append(data)\n",
    "            except StopIteration:\n",
    "                continue_iterator = False\n",
    "                break\n",
    "        random.shuffle(data_list)\n",
    "        text_list = []\n",
    "        if continue_iterator:\n",
    "            for data in data_list:\n",
    "                text_list.append(data)\n",
    "\n",
    "            yield text_list\n",
    "        else:\n",
    "            return StopIteration\n",
    "# 生成数据集对应的标签集以及样本总数\n",
    "def build_label_set_and_sample_num(input_path, output_path):\n",
    "    label_set = set()\n",
    "    sample_num = 0\n",
    "    with open(input_path, 'r', encoding=\"utf-8\") as input_file:\n",
    "        for line in tqdm(input_file):\n",
    "            json_data = json.loads(line)\n",
    "            label_set.add(json_data[\"label\"])\n",
    "            sample_num += 1\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"UTF-8\") as output_file:\n",
    "        record = {\"label_list\": sorted(list(label_set)), \"total_num\": sample_num}\n",
    "        json.dump(record, output_file, ensure_ascii=False)\n",
    "\n",
    "        return record[\"label_list\"], record[\"total_num\"]\n",
    "class MyBertEncoder(nn.Module):\n",
    "    def __init__(self,tokenizer_path,finetuned_bert_path):\n",
    "        super(MyBertEncoder,slef).__init__()\n",
    "        model_class,tokenizer_class = tfs.BertModel,tfs.BertTokenizer\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(tokenizer_path)\n",
    "        self.bert = torch.load(finetuned_bert_path)\n",
    "    def forward(self,batch_sentences):\n",
    "        batch_tokenzied = self.tokenizer.batch_encode_plus(batch_sentences,add_special_tokens=True,\n",
    "                                                            max_length=512,pad_to_max_length=True)\n",
    "        input_ids = torch.tensor(batch_tokenzied['input_ids']).cuda()\n",
    "        token_type_ids = torch.tensor(batch_tokenized['token_type_ids']).cuda()\n",
    "        attention_mask = torch.tensor(batch_tokenized['attention_mask']).cuda()\n",
    "        \n",
    "        bert_output = self.bert(input_ids=input_ids,token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        bert_cls_hidden_state = bert_output[0][:,0,:]\n",
    "        \n",
    "        return bert_cls_hidden_state\n",
    "def build_pu_data():\n",
    "    pos_data_iter = get_bert_iterator_batch(POSITIVE_TRAIN_FILE_PATH,batch_size = BATCH_SIZE)\n",
    "    unlabeled_data_iter = get_bert_iterator_batch(UNLABELED_TRAIN_FILE_PATH, batch_size=BATCH_SIZE*2)\n",
    "    torch.cuda.set_device(0)\n",
    "    encoder = MyBertEncoder(BERT_TOKENZIER_PATH, FINETUNED_BERT_ENCODER_PATH)\n",
    "    encoder.eval()\n",
    "    X, y = None, None\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for pos_batch, unlabeled_batch in tqdm(zip(pos_data_iter, unlabeled_data_iter)):\n",
    "            encoded_pos = np.array(encoder(pos_batch).tolist())\n",
    "            encoded_unlabeled = np.array(encoder(unlabeled_batch).tolist())\n",
    "            if i == 0:\n",
    "                X = np.concatenate([encoded_pos, encoded_unlabeled], axis=0)\n",
    "                y = np.concatenate([np.full(shape=encoded_pos.shape[0], fill_value=1, dtype=np.int),\n",
    "                                    np.full(shape=encoded_unlabeled.shape[0], fill_value=0, dtype=np.int)])\n",
    "            else:\n",
    "                X = np.concatenate([X, encoded_pos, encoded_unlabeled], axis=0)\n",
    "                y = np.concatenate([y, np.full(shape=encoded_pos.shape[0], fill_value=1, dtype=np.int),\n",
    "                                    np.full(shape=encoded_unlabeled.shape[0], fill_value=0, dtype=np.int)])\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        np.save(PU_DATA_TEXT_SAVE_PATH, X)\n",
    "        np.save(PU_DATA_LABEL_SAVE_PATH, y)\n",
    "        print(\"PU data build successfully...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
